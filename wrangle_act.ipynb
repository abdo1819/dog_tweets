{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time , os ,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view the tweet text \n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table of contents :\n",
    "   - [data gathering](#data-gathering)\n",
    "   - [assising result](#assising-result)\n",
    "   - [analysis](#analysis)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data gathering\n",
    "\n",
    "* read from `twitter-archive-enhanced.csv`\n",
    "* download `image_predictions.tsv`\n",
    "\n",
    "* use tweepy to load\n",
    "    - retweet count\n",
    "    - like count\n",
    "    - ...\n",
    "* save tweater data to `tweet_json.txt` and load them as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read from `twitter-archive-enhanced.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive = pd.read_csv('twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download `image_predictions.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file if not already downloades\n",
    "if not os.path.exists('image_predictions.tsv'):\n",
    "    response = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "    with open('image_predictions.tsv','wb') as file:\n",
    "        file.write(response.content)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions = pd.read_csv('image_predictions.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use tweepy to load twtweets data**\n",
    "    - retweet count\n",
    "    - like count\n",
    "    - ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'JJW6Y1yUkRzWORLJp3AsnoNS0'\n",
    "consumer_secret = '7HOeyP0Q8f749qFE2E6hIACoFwgHYCv2nOIbLpblcIqJStZb7o'\n",
    "access_token = '3084174784-vfOMmfQcZTFl3m5loSs5BQfYcfxHg3epEKcUAxH'\n",
    "access_secret = '5tEPXKzPcxHBRaZzM0mzb7euKG2vZrE5uOkbO95BSoNfD'\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "## http://docs.tweepy.org/en/v3.2.0/api.html#API\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://tweepy.readthedocs.io/en/latest/api.html#API.get_status\n",
    "- http://docs.tweepy.org/en/latest/extended_tweets.html?highlight=tweet_mode#extended-mode\n",
    "- http://docs.tweepy.org/en/latest/extended_tweets.html?highlight=tweet_mode#examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_ids = []\n",
    "\n",
    "\n",
    "if not os.path.exists('tweet_json.txt'):\n",
    "    with open('tweet_json.txt','w') as tweets_file:\n",
    "        for tweet_id in tqdm(twitter_acrchive.tweet_id):\n",
    "            try:\n",
    "                tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "                json.dump(tweet._json, tweets_file)\n",
    "                tweets_file.write('\\n')\n",
    "            except Exception as e:\n",
    "                failed_ids.append(tweet_id)\n",
    "    # summery \n",
    "    num_ids = len(twitter_acrchive.tweet_id)\n",
    "    num_failed = len(failed_ids)\n",
    "    num_success = num_ids - num_failed\n",
    "\n",
    "    print('--------------------------\\n')\n",
    "    print('sucessfull = ' + str(num_success))\n",
    "    print('failed = ' + str(num_failed))\n",
    "    print('------ failed tweets ---------------------')\n",
    "    print(failed_ids)\n",
    "else:\n",
    "    print('file already exist please remove it if not complate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* save tweater data to `tweet-json.txt` \n",
    "* convert data to dict then to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open('tweet_json.txt','r') as json_file:\n",
    "    for line in json_file.readlines():\n",
    "        data = json.loads(line)\n",
    "        data['favorite_count']\n",
    "        \n",
    "        rows.append({'tweet_id': data['id'],\n",
    "                     'retweet_count':  data['retweet_count'],\n",
    "                     'favorite_count': data['favorite_count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_extra = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save loaded data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original data\n",
    "image_predictions.to_csv('image_predictions.csv', index=False)\n",
    "twitter_extra.to_csv('twitter_extra.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check any error in rating , dog stages\n",
    "\n",
    "quate\n",
    "```\n",
    "I extracted this data programmatically, but I didn't do a very good job. The ratings probably aren't all correct. Same goes for the dog names and probably dog stages .\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [assess_1](#assising-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_score =  twitter_acrchive.rating_numerator.astype(str) + '/' + twitter_acrchive.rating_denominator.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweete_score = twitter_acrchive.text.str.extract(r'(\\d+\\/\\d+)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive.loc[calculated_score!=tweete_score,['tweet_id','text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `835246439529840640` has two rating like text\n",
    "\n",
    "* `826598799820865537` is  comment , the rated was in other post\n",
    "\n",
    "https://twitter.com/dog_rates/status/826598799820865537"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_2](#assising-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive[['rating_numerator','rating_denominator']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_numerator = twitter_acrchive[(twitter_acrchive.rating_numerator < 10) | (twitter_acrchive.rating_numerator > 20)]\n",
    "strange_numerator[['tweet_id','text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rating with fraction doesn't get the whole number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_3](#assising-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_denominator = twitter_acrchive[twitter_acrchive.rating_denominator != 10]\n",
    "strange_denominator[['tweet_id','text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `810984652412424192` has no rating \n",
    "* `666287406224695296`,`835246439529840640` ..  has two rating like text\n",
    "* `758467244762497024` denomrator is not 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check other column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_4](#assising-result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- complex values[url, string] in source coloumn \n",
    "- null values in [retweeted_status_id\tretweeted_status_user_id\tretweeted_status_timestamp]\n",
    ", [in_reply_to_status_id\tin_reply_to_user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_5](#assising-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_acrchive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wrong dtypes: `[timestamp,retweeted_status_timestamp]`\n",
    "- dog type in represented in multiple columns (doggo  floofer pupper puppo)\n",
    "- column source is not descriptive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_6](#assising-result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_predictions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- image prediction multible coulmns for beerd \n",
    "- number of tweats is higher than number of images\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_7](#assising-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions[~image_predictions.p1_dog & ~image_predictions.p2_dog & ~image_predictions.p3_dog]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some images has no dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_8](#assising-result)\n",
    "* programitically extract score for multible image tweats is requires manual image inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [asses_9](#assising-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_extra.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tweet extra has related data to tweater_archive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assising result\n",
    "\n",
    "[analysis](#analysis)\n",
    "\n",
    "* note: asses anchor is written after the observation\n",
    "## ROUND 1\n",
    "### messy\n",
    "- [x] tweet extra has related data to tweater_archive \\[[assessing](#asses_9)\\] \\[[tide](#tide_1)\\]\n",
    "- [x] dog type in represented in column names (doggo  floofer pupper puppo)  \\[[assessing](#asses_5)\\] \\[[tide](#tide_2)\\]\n",
    "- [x] name column `source`  is not descriptive \\[[assessing](#asses_5)\\] \\[[tide](#tide_4)\\]\n",
    "- [x] image prediction multible coulmns for beard `hard analysis` \\[[assessing](#asses_6)\\] \\[[tide](#tide_3)\\]\n",
    "- [x] dog breed can be added to Twitter archive  \\[[tide](#tide_5)\\]\n",
    "\n",
    "\n",
    "### dirty\n",
    "* tweater_archive\n",
    "    - [x] complex values in source coloumn \\[[assessing](#asses_4)\\] \\[[clean](#clean_1)\\]\n",
    "    - [x] rating with fraction takes fraction only _ex.. `9.5/10` is saved as `5/10`\n",
    "    \\[[assessing](#asses_2)\\] \\[[clean](#clean_2)\\]\n",
    "        - [x] `181` tweats that have retweet_status and `78` replays \\[[clean](#clean_4)\\]\n",
    "    ```\n",
    "    You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "    ```\n",
    "    - [x] stages[doggo , floofer ..] and name columns use string `None` or `none` to represent null value [solved in tidy stage] \n",
    "    - [x] wrong dtypes  \\[[assessing](#asses_5)\\] \\[[clean](#clean_6)\\]\n",
    "        * tweater_archive: timestamp, *after data tidtidying* [bread,type]\n",
    "\n",
    "* image preditictions\n",
    "    - [x] some images has no dogs \\[[assessing](#asses_7)\\] \\[[clean](#clean_5)\\]\n",
    "    - [x] number of tweets is higher than number of predicted images \\[[assessing](#asses_6)\\] \\[[clean](#clean_7)\\]\n",
    "    - [x] dog bread is catagorial dtype \\[[clean](#clean_6)\\]\n",
    "\n",
    "\n",
    "# ROUND 2 _after cleaning\n",
    "### dirty\n",
    "- [x] some dogs has multipe dog_stage \\[[assessing](#asses_2_1)\\] \\[[clean](#clean_8)\\]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleanning\n",
    "\n",
    "## make copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean =  twitter_acrchive.copy()\n",
    "twitter_extra_clean = twitter_extra.copy()\n",
    "image_predictions_clean = image_predictions.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tidness issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tide_1](#assising-result)\n",
    "\n",
    "### tweet extra has related data to twitter_archive \n",
    "\n",
    "\n",
    "solution:\n",
    "* merge the two data frames with tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean = twitter_acrchive_clean.merge(twitter_extra, on='tweet_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tide_2](#assising-result)\n",
    "### dog type is represented in column name (doggo  floofer pupper puppo)\n",
    "\n",
    "\n",
    "\n",
    "solution:\n",
    "- create column named `type` that holds this value \n",
    "- drop those columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract other columns names\n",
    "id_list = list(twitter_acrchive_clean.columns)\n",
    "dog_types = ['doggo','floofer','pupper','puppo']\n",
    "id_list = [x for x in id_list if x not in dog_types]\n",
    "\n",
    "# melt types columns\n",
    "dog_type = twitter_acrchive_clean.melt(id_vars=id_list,value_name='dog_type')\n",
    "dog_type = dog_type[['tweet_id','dog_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the records with None and add the new column \n",
    "dog_type = dog_type[dog_type.dog_type != 'None']\n",
    "twitter_acrchive_clean = twitter_acrchive_clean.merge(dog_type,on='tweet_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.drop(columns=dog_types, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### asses_2_1\n",
    "-------------------------------------\n",
    "# dirty data\n",
    "* the count increased from `2356` to `2368` \n",
    "    - some dogs has multipe types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tide_3](#assising-result)\n",
    "\n",
    "### image prediction represent  beard in multible coulmns\n",
    "\n",
    "* take the most confidant dog bread\n",
    "    1. change false in `p1,p2,p3` coulmns with null\n",
    "    2. use `combine` to get columns with non null value betwean `p1,p2`\n",
    "    3. reapeat 2 for `last_result,p3`\n",
    "    4. drop useless coulmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace false breed with null\n",
    "image_predictions_clean.loc[~image_predictions_clean.p1_dog,['p1']] = np.nan\n",
    "image_predictions_clean.loc[~image_predictions_clean.p1_dog,['p1_conf']] = np.nan\n",
    "\n",
    "\n",
    "image_predictions_clean.loc[~image_predictions_clean.p2_dog,['p2']] = np.nan\n",
    "image_predictions_clean.loc[~image_predictions_clean.p2_dog,['p2_conf']] = np.nan\n",
    "\n",
    "image_predictions_clean.loc[~image_predictions_clean.p3_dog,['p3']] = np.nan\n",
    "image_predictions_clean.loc[~image_predictions_clean.p3_dog,['p3_conf']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose non false breed p1,p2\n",
    "image_predictions_clean['breed'] = image_predictions_clean.p1.combine_first(image_predictions_clean.p2)\n",
    "image_predictions_clean['breed_confidance'] = image_predictions_clean.p1_conf.combine_first(image_predictions_clean.p2_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose non false breed last_result,p3\n",
    "image_predictions_clean['breed'] = image_predictions_clean.breed.combine_first(image_predictions_clean.p3)\n",
    "image_predictions_clean['breed_confidance'] = image_predictions_clean.breed_confidance.combine_first(image_predictions_clean.p3_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* naming was missy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.drop(columns=['p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog'],\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [tide_4](#assising-result)\n",
    "\n",
    "\n",
    "## column source is not descriptive\n",
    "- rename to `source_device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.rename(columns={'source':'source_device'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ditry data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_1](#assising-result)\n",
    "\n",
    "###  complex values in source coloumn \n",
    "- select text in <h> tag body \n",
    "- convert to simpler values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweete_source = twitter_acrchive_clean.source_device.str.extract(r'(>.*<)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.source_device = tweete_source.str[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_simple_source = {\n",
    "    'Twitter for iPhone' : 'iPhone',\n",
    "    'Twitter Web Client' : 'Web Client',\n",
    "    'Vine - Make a Scene' : 'Vine',\n",
    "    'TweetDeck' : 'TweetDeck'\n",
    "}\n",
    "twitter_acrchive_clean.source_device.replace(map_simple_source, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.source_device.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rating problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_2](#assising-result)\n",
    "\n",
    "#### rating with fraction takes fraction only _ex.. `9.5/10` is saved as `5/10`_\n",
    "\n",
    "\n",
    "### sol:\n",
    "* use regex expration that take the point into acount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observed cases:\n",
    "    - 9.5/10 \n",
    "    - ...10/10 \n",
    "    - 13/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweete_score = twitter_acrchive_clean.text.str.extract(r'(\\d+\\.?\\d*\\/\\d+\\.?\\d*)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean[['rating_numerator','rating_denominator']] =  tweete_score.str.split('/',expand=True).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.loc[twitter_acrchive_clean.rating_numerator%1!=0,\n",
    "                           ['text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_3](#assising-result)\n",
    "\n",
    "#### tweets with rating like value \n",
    "\n",
    "## define\n",
    "\n",
    "#### double rating cases\n",
    "##### real doubles or 10/10 => take the first one only\n",
    "- two dogs two ratings \n",
    "- one dog two 10/10\n",
    "\n",
    "##### fake double => take last rating\n",
    "- 24/7\n",
    "- joking then rate\n",
    "- contain date 4/20\n",
    "\n",
    "## sol\n",
    "\n",
    "#### sol [real double]:\n",
    "- select tweats with `and` or `&` or `10/10`\n",
    "- choose the first rate as the right rating\n",
    "\n",
    "\n",
    "\n",
    "#### sol [fake double]:\n",
    "- select tweats without `and` or `&` or `10/10`\n",
    "- choose the last rate as the right rating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_all = twitter_acrchive_clean.text.str.extractall(r'(\\d+\\.?\\d*\\/\\d+\\.?\\d*)')\n",
    "double_rated = twitter_acrchive_clean.iloc[match_all.xs(1,level=1).index]\n",
    "double_rated = double_rated[['tweet_id','name','text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_double = double_rated[(double_rated.text.str.find('and') < 0) &\n",
    "                            (double_rated.text.str.find('&') < 0) &\n",
    "                            (double_rated.text.str.find('10/10') < 0)]\n",
    "\n",
    "real_double = double_rated.drop(fake_double.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the second rate for fake doubles\n",
    "second_valid = match_all.xs(1,level=1).loc[fake_double.index][0]\n",
    "second_index = second_valid.index\n",
    "second_nom_dem = second_valid.str.split('/',expand=True).astype(float)\n",
    "\n",
    "twitter_acrchive_clean.loc[second_index,['rating_numerator']] = second_nom_dem[0]\n",
    "twitter_acrchive_clean.loc[second_index,['rating_denominator']] = second_nom_dem[1]\n",
    "\n",
    "# for real double first tweet is already taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake_doubles\n",
    "twitter_acrchive_clean.loc[second_index,['tweet_id','text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_doubles\n",
    "twitter_acrchive_clean.loc[real_double.index, ['tweet_id','name','text','rating_numerator','rating_denominator']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_5](#assising-result)\n",
    "\n",
    "### some images has no dogs \n",
    "\n",
    "remove tweeets with null value at dog breed (the AI model coudn't detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_clean = image_predictions_clean[image_predictions_clean.breed.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_4](#assising-result)\n",
    "\n",
    "### `181` tweats that have retweet_status,  `78` replies\n",
    "\n",
    "- remove retweats and replies\n",
    "- remove there associated coulmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_index = twitter_acrchive_clean[twitter_acrchive_clean.retweeted_status_id.notnull()].index\n",
    "twitter_acrchive_clean.drop(labels=retweet_index,inplace=True)\n",
    "\n",
    "replay_index = twitter_acrchive_clean[twitter_acrchive_clean.in_reply_to_status_id.notnull()].index\n",
    "twitter_acrchive_clean.drop(labels=replay_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.drop(columns=['in_reply_to_status_id','in_reply_to_user_id'],inplace=True)\n",
    "twitter_acrchive_clean.drop(columns=['retweeted_status_id',\n",
    "                                     'retweeted_status_user_id',\n",
    "                                     'retweeted_status_timestamp'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_6](#assising-result)\n",
    "\n",
    "## wrong dtypes\n",
    "    * timedate: timestamp\n",
    "    * catagorial : dog_type ,source_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.timestamp = pd.to_datetime(twitter_acrchive_clean.timestamp)\n",
    "twitter_acrchive_clean.dog_type = twitter_acrchive_clean.dog_type.astype('category')\n",
    "twitter_acrchive_clean.source_device = twitter_acrchive_clean.source_device.astype('category')\n",
    "image_predictions_clean.breed = image_predictions_clean.breed.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_7](#assising-result)\n",
    "\n",
    "### number of tweets is higher than number of predicted images\n",
    "\n",
    "define : there are tweets that has no images or images without dogs we nee to exclude those from analysis\n",
    "\n",
    "sol : \n",
    "- remove tweets whose id doesn't exist in image prediction data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean = twitter_acrchive_clean[twitter_acrchive_clean.tweet_id.isin(image_predictions_clean.tweet_id)]\n",
    "image_predictions_clean = image_predictions_clean[image_predictions_clean.tweet_id.isin(twitter_acrchive_clean.tweet_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_predictions_clean.shape)\n",
    "print(twitter_acrchive_clean.shape)\n",
    "# the difrance in size is result from duplicate in twitter_acrchive_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [clean_8](#assising-result)\n",
    "\n",
    "### some dogs has multipe dog_stages\n",
    "\n",
    "define : in tweet text we rate dogs may mention two dog stages which result in duplcate in data\n",
    "\n",
    "sol : \n",
    "- remove duplicate tweets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.drop_duplicates(subset=['tweet_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if duplicate make them equal\n",
    "print(image_predictions_clean.shape)\n",
    "print(twitter_acrchive_clean.shape)\n",
    "\n",
    "twitter_acrchive_clean.tweet_id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master = twitter_acrchive_clean.merge(image_predictions_clean,on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_master.to_csv('twitter_archive_master.csv',index=False)\n",
    "twitter_acrchive_clean.to_csv('twitter_acrchive_clean.csv',index=False)\n",
    "image_predictions_clean.to_csv('image_predictions_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis\n",
    "- avrage rating for dog types, dogbreed\n",
    "- most retweated/loved type\n",
    "- is rating random ? \n",
    "- value count for types\n",
    "- do people agree ? the likes compared to rating\n",
    "- number of photos effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_by_type = twitter_acrchive_clean.groupby('dog_type').mean()\n",
    "means_by_type = means_by_type[['rating_numerator','retweet_count','favorite_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_by_type.plot.bar(subplots=True,figsize=(8,8),logy=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.rating_numerator[twitter_acrchive_clean.rating_numerator < 20].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* seems scwed a bit the right but overall seems a normal distubtion not random values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value count for types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.dog_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_acrchive_clean.groupby(['source_device','dog_type']).count()['tweet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
